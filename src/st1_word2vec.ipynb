{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-task 1 with Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run import_data.ipynb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import multilabel_confusion_matrix,recall_score, accuracy_score, hamming_loss, precision_score, f1_score\n",
    "from utils.evaluation import evaluate_pipeline_x_validation\n",
    "from embeddings.word2vec import Word2VecMean\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "word2vec = Word2VecMean()\n",
    "\n",
    "# creating a pipeline to run on each classifier.\n",
    "pipeline = Pipeline([\n",
    "    ('w2v', word2vec),\n",
    "    ('clf', ClfSwitcher())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Experiences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [02:04<02:04, 62.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unbalanced1</td>\n",
       "      <td>0.327165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unbalanced2</td>\n",
       "      <td>0.294734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  f1-micro\n",
       "0  unbalanced1  0.327165\n",
       "1  unbalanced2  0.294734"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weigth = [None,'balanced']\n",
    "\n",
    "estimators = [DecisionTreeClassifier(class_weight=weigth) for weigth in class_weigth]\n",
    "\n",
    "grid = ParameterGrid({\n",
    "    'clf__estimator': estimators,\n",
    "})\n",
    "\n",
    "models = ['unbalanced1', 'unbalanced2', 'balanced1', 'balanced2']\n",
    "\n",
    "evaluate_pipeline_x_validation(pipeline, grid, models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro score for DecisionTree is 0.29752066115702475\n",
      "F1-macro score for DecisionTree is 0.10214419814585282\n"
     ]
    }
   ],
   "source": [
    "decision_tree_best = grid[0]\n",
    "\n",
    "pipeline.set_params(**decision_tree_best)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"F1-micro score for DecisionTree is {f1_score(y_test, y_pred, average='micro', zero_division=0)}\")\n",
    "print(f\"F1-macro score for DecisionTree is {f1_score(y_test, y_pred, average='macro', zero_division=0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:29<00:00, 52.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraMaxNone</td>\n",
       "      <td>0.307583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ExtraMax5</td>\n",
       "      <td>0.301104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ExtraMax10</td>\n",
       "      <td>0.302966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraMax20</td>\n",
       "      <td>0.300559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  f1-micro\n",
       "0  ExtraMaxNone  0.307583\n",
       "1     ExtraMax5  0.301104\n",
       "2    ExtraMax10  0.302966\n",
       "3    ExtraMax20  0.300559"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = [None,5,10,20]\n",
    "\n",
    "estimators = [ExtraTreeClassifier(max_features=feat) for feat in max_features]\n",
    "\n",
    "grid = ParameterGrid({\n",
    "    'clf__estimator': estimators\n",
    "})\n",
    "\n",
    "models = ['ExtraMaxNone', 'ExtraMax5', 'ExtraMax10', 'ExtraMax20']\n",
    "\n",
    "evaluate_pipeline_x_validation(pipeline, grid, models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro score for Extra Trees is 0.2581560283687943\n",
      "F1-macro score for Extra Trees is 0.07578959353720192\n"
     ]
    }
   ],
   "source": [
    "extra_tree_best = grid[0]\n",
    "\n",
    "pipeline.set_params(**extra_tree_best)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"F1-micro score for Extra Trees is {f1_score(y_test, y_pred, average='micro', zero_division=0)}\")\n",
    "print(f\"F1-macro score for Extra Trees is {f1_score(y_test, y_pred, average='macro', zero_division=0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:45<00:00, 56.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knn5_</td>\n",
       "      <td>0.447417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Knn10_</td>\n",
       "      <td>0.443302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Knn20_</td>\n",
       "      <td>0.454261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Knn50_</td>\n",
       "      <td>0.457072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  f1-micro\n",
       "0   Knn5_  0.447417\n",
       "1  Knn10_  0.443302\n",
       "2  Knn20_  0.454261\n",
       "3  Knn50_  0.457072"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors = [5,10,20,50]\n",
    "\n",
    "estimators = [KNeighborsClassifier(n_neighbors=n) for n in n_neighbors]\n",
    "\n",
    "grid = ParameterGrid({\n",
    "    'clf__estimator': estimators\n",
    "})\n",
    "\n",
    "models = ['Knn5_', 'Knn10_', 'Knn20_', 'Knn50_']\n",
    "\n",
    "evaluate_pipeline_x_validation(pipeline, grid, models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro score for KNN is 0.4451718494271686\n",
      "F1-macro score for KNN is 0.07417863362450243\n"
     ]
    }
   ],
   "source": [
    "knn_best = grid[2]\n",
    "\n",
    "pipeline.set_params(**knn_best)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"F1-micro score for KNN is {f1_score(y_test, y_pred, average='micro', zero_division=0)}\")\n",
    "print(f\"F1-macro score for KNN is {f1_score(y_test, y_pred, average='macro', zero_division=0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [04:23<04:23, 65.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m1</td>\n",
       "      <td>0.316847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m2</td>\n",
       "      <td>0.307348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m3</td>\n",
       "      <td>0.454261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m4</td>\n",
       "      <td>0.379484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  f1-micro\n",
       "0    m1  0.316847\n",
       "1    m2  0.307348\n",
       "2    m3  0.454261\n",
       "3    m4  0.379484"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = [BinaryRelevance(classifier = DecisionTreeClassifier()),\n",
    "              BinaryRelevance(classifier = ExtraTreeClassifier()),\n",
    "              BinaryRelevance(classifier = KNeighborsClassifier(n_neighbors=20)),\n",
    "              BinaryRelevance(classifier = GaussianNB())]\n",
    "\n",
    "grid = ParameterGrid({\n",
    "    'clf__estimator': classifier\n",
    "})\n",
    "\n",
    "models = ['m1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8']\n",
    "\n",
    "evaluate_pipeline_x_validation(pipeline, grid, models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro score for BinaryRelevance with model M1 is 0.2949547218628719\n",
      "F1-macro score for BinaryRelevance with model M1 is 0.09874350751654899\n",
      "[0.         0.0625     0.         0.         0.         0.0952381\n",
      " 0.16       0.06666667 0.         0.56481481 0.         0.34375\n",
      " 0.         0.         0.         0.         0.125      0.27536232\n",
      " 0.         0.0952381 ]\n",
      "F1-micro score for BinaryRelevance with model M2 is 0.32697547683923706\n",
      "F1-macro score for BinaryRelevance with model M2 is 0.09604054198755371\n",
      "[0.         0.         0.         0.         0.         0.15\n",
      " 0.13043478 0.         0.         0.60576923 0.         0.34090909\n",
      " 0.         0.         0.         0.         0.11764706 0.32692308\n",
      " 0.         0.11764706]\n",
      "F1-micro score for BinaryRelevance with model M3 is 0.4451718494271686\n",
      "F1-macro score for BinaryRelevance with model M3 is 0.07417863362450243\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.59615385 0.         0.40625\n",
      " 0.         0.         0.         0.         0.         0.34090909\n",
      " 0.         0.        ]\n",
      "F1-micro score for BinaryRelevance with model M4 is 0.3602251407129456\n",
      "F1-macro score for BinaryRelevance with model M4 is 0.19424292541202226\n",
      "[0.0952381  0.09302326 0.         0.33333333 0.05882353 0.21666667\n",
      " 0.21666667 0.125      0.19512195 0.65686275 0.03571429 0.33333333\n",
      " 0.         0.         0.         0.         0.17857143 0.36170213\n",
      " 0.075      0.09615385]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "for index, params in enumerate(grid, start=1):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f\"F1-micro score for BinaryRelevance with model M{index} is {f1_score(y_test, y_pred, average='micro', zero_division=0)}\")\n",
    "    print(f\"F1-macro score for BinaryRelevance with model M{index} is {f1_score(y_test, y_pred, average='macro', zero_division=0)}\")\n",
    "    class_recall = precision_score(y_true=y_test, y_pred=y_pred, average=None, zero_division=0)\n",
    "    print(class_recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [02:47<02:47, 55.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1-micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m1</td>\n",
       "      <td>0.327410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m2</td>\n",
       "      <td>0.450436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m3</td>\n",
       "      <td>0.424347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  f1-micro\n",
       "0    m1  0.327410\n",
       "1    m2  0.450436\n",
       "2    m3  0.424347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = [LabelPowerset(classifier = DecisionTreeClassifier()),\n",
    "              LabelPowerset(classifier = KNeighborsClassifier(n_neighbors=20)),\n",
    "              LabelPowerset(classifier = GaussianNB())]\n",
    "\n",
    "grid = ParameterGrid({\n",
    "    'clf__estimator': classifier\n",
    "})\n",
    "\n",
    "\n",
    "models = ['m1', 'm2', 'm3', 'm4', 'm5', 'm6']\n",
    "\n",
    "evaluate_pipeline_x_validation(pipeline, grid, models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro score for Label Powerset with model M1 is 0.3046153846153846\n",
      "F1-macro score for Label Powerset with model M1 is 0.08990277050464493\n",
      "[0.         0.16666667 0.         0.         0.         0.07142857\n",
      " 0.06666667 0.         0.         0.6344086  0.         0.24242424\n",
      " 0.         0.         0.         0.         0.16666667 0.25757576\n",
      " 0.         0.3       ]\n",
      "F1-micro score for Label Powerset with model M2 is 0.41486068111455116\n",
      "F1-macro score for Label Powerset with model M2 is 0.08276372750056961\n",
      "[0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.60305344 0.         0.34482759\n",
      " 0.         0.         0.         0.         0.         0.27272727\n",
      " 0.         0.        ]\n",
      "F1-micro score for Label Powerset with model M3 is 0.3999999999999999\n",
      "F1-macro score for Label Powerset with model M3 is 0.09062047932232711\n",
      "[0.         0.         0.         0.         0.         0.42857143\n",
      " 0.5        0.         0.         0.65625    0.         0.375\n",
      " 0.         0.         0.         0.         0.         0.4137931\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "for index, params in enumerate(grid, start=1):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f\"F1-micro score for Label Powerset with model M{index} is {f1_score(y_test, y_pred, average='micro', zero_division=0)}\")\n",
    "    print(f\"F1-macro score for Label Powerset with model M{index} is {f1_score(y_test, y_pred, average='macro', zero_division=0)}\")\n",
    "    class_recall = precision_score(y_true=y_test, y_pred=y_pred, average=None, zero_division=0)\n",
    "    print(class_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adpt-yLsCzLSp-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a72d8dc4051dd080fc153a5760538b919c0abbb9227ab39a2d2f4f67b8d7524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
